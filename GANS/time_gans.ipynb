{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "class GalaxyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx], dtype=torch.float32)\n",
    "    \n",
    "def get_dataloader(data, batch_size=32, shuffle=True):\n",
    "    dataset = GalaxyDataset(data)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def fit_scaler(data):\n",
    "    fluxes = data[:, :, 1:]  # Only fluxes (columns 1-10)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(fluxes.reshape(-1, fluxes.shape[-1]))\n",
    "    return scaler\n",
    "\n",
    "def normalize_data(data, scaler):\n",
    "    wavelengths = data[:, :, 0:1]\n",
    "    fluxes = data[:, :, 1:]\n",
    "    normalized_fluxes = scaler.transform(fluxes.reshape(-1, fluxes.shape[-1]))\n",
    "    return np.concatenate([wavelengths, normalized_fluxes.reshape(data.shape[0], data.shape[1], -1)], axis=2)\n",
    "\n",
    "def inverse_normalize_data(data, scaler):\n",
    "    fluxes = data[:, :, 1:]\n",
    "    original_fluxes = scaler.inverse_transform(fluxes.reshape(-1, fluxes.shape[-1]))\n",
    "    return original_fluxes.reshape(data.shape[0], data.shape[1], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        attn_weights = torch.softmax(self.attn(hidden_states), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * hidden_states, dim=1)\n",
    "        return context_vector\n",
    "\n",
    "class ComplexGenerator(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(ComplexGenerator, self).__init__()\n",
    "        # Using a deeper RNN architecture with LSTMs\n",
    "        self.rnn = nn.LSTM(latent_dim + 1, hidden_dim, batch_first=True, num_layers=5, bidirectional=True, dropout=0.3)\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.3)  # Attention mechanism\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Increased to handle bidirectional output\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, output_dim)\n",
    "\n",
    "        # Additional layers for increased complexity\n",
    "        self.residual_fc = nn.Sequential(\n",
    "            nn.Linear(output_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, output_dim)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, z, wavelengths):\n",
    "        if wavelengths.dim() == 2:\n",
    "            wavelengths = wavelengths.unsqueeze(-1)\n",
    "        z = torch.cat([z, wavelengths], dim=-1)\n",
    "        h, _ = self.rnn(z)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        h = h.permute(1, 0, 2)  # (seq_len, batch, hidden_dim * 2)\n",
    "        h_attn, _ = self.attn(h, h, h)  # Self-attention\n",
    "        h_attn = h_attn.permute(1, 0, 2)  # (batch, seq_len, hidden_dim * 2)\n",
    "\n",
    "        h = self.dropout(h_attn)  # Apply dropout\n",
    "        h = torch.relu(self.fc1(h))\n",
    "        h = torch.relu(self.ln1(self.fc2(h)))\n",
    "        output = self.fc3(h)\n",
    "\n",
    "        # Apply residual connection\n",
    "        output = output + self.residual_fc(output)\n",
    "        return output\n",
    "\n",
    "class ComplexDiscriminator(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ComplexDiscriminator, self).__init__()\n",
    "        # Deeper RNN with Bidirectional LSTM\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=5, dropout=0.3)\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim * 2, num_heads=4, dropout=0.3)  # Attention mechanism\n",
    "        self.dropout = nn.Dropout(0.4)  # Increase dropout rate\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.fc3 = nn.Linear(hidden_dim // 2, hidden_dim // 4)\n",
    "        self.ln1 = nn.LayerNorm(hidden_dim // 2)\n",
    "        self.ln2 = nn.LayerNorm(hidden_dim // 4)\n",
    "        self.fc4 = nn.Linear(hidden_dim // 4, output_dim)\n",
    "\n",
    "        # Additional Residual Blocks\n",
    "        self.residual_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 4, hidden_dim // 4)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.rnn(x)\n",
    "\n",
    "        # Apply attention mechanism\n",
    "        h = h.permute(1, 0, 2)  # (seq_len, batch, hidden_dim * 2)\n",
    "        h_attn, _ = self.attn(h, h, h)  # Self-attention\n",
    "        h_attn = h_attn.permute(1, 0, 2)  # (batch, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # Pool across time steps (mean) and pass through the rest of the network\n",
    "        h = self.dropout(h_attn.mean(dim=1))  # Apply dropout after attention\n",
    "        h = torch.relu(self.fc1(h))\n",
    "        h = torch.relu(self.ln1(self.fc2(h)))\n",
    "        h = torch.relu(self.ln2(self.fc3(h)))\n",
    "        \n",
    "        # Residual connection\n",
    "        h = h + self.residual_fc(h)\n",
    "        \n",
    "        return torch.sigmoid(self.fc4(h))\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, latent_dim=32):\n",
    "        super(EmbeddingNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)\n",
    "        self.latent = nn.Linear(hidden_dim, latent_dim)\n",
    "        nn.init.xavier_uniform_(self.latent.weight)\n",
    "        nn.init.zeros_(self.latent.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return self.latent(x)\n",
    "\n",
    "class RecoveryNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(RecoveryNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.rnn = nn.GRU(hidden_dim, output_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.fc(h).unsqueeze(1)\n",
    "        output, _ = self.rnn(h.repeat(1, h.shape[1], 1))\n",
    "        return output\n",
    "\n",
    "class TimeGAN:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, device):\n",
    "        self.device = device\n",
    "        self.embedding_net = EmbeddingNetwork(input_dim, hidden_dim, latent_dim).to(device)\n",
    "        self.recovery_net = RecoveryNetwork(latent_dim, hidden_dim, input_dim).to(device)\n",
    "        self.generator_net = ComplexGenerator(latent_dim, hidden_dim, 10000).to(device)  # Predict 10 fluxes\n",
    "        self.discriminator_net = ComplexDiscriminator(10000, hidden_dim, 1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(discriminator, real_data, fake_data):\n",
    "    \"\"\"\n",
    "    Compute the gradient penalty for WGAN-GP.\n",
    "\n",
    "    Parameters:\n",
    "    - discriminator: The discriminator network.\n",
    "    - real_data: Real data samples.\n",
    "    - fake_data: Fake data generated by the generator.\n",
    "\n",
    "    Returns:\n",
    "    - gradient_penalty: A scalar value for the gradient penalty term.\n",
    "    \"\"\"\n",
    "    # Interpolate between real and fake data\n",
    "    alpha = torch.rand(real_data.size(0), 1, 1).to(real_data.device)\n",
    "    alpha = alpha.expand_as(real_data)\n",
    "    \n",
    "    # Create interpolated samples\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates.requires_grad_(True)\n",
    "    \n",
    "    # Compute the discriminator output for interpolated samples\n",
    "    d_interpolates = discriminator(interpolates)\n",
    "    \n",
    "    # Compute gradients with respect to the interpolated samples\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(d_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "    \n",
    "    # Compute the L2 norm of the gradients\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def train_timegan(train_loader, scaler, device, epochs=1000, lambda_gp=10):\n",
    "    input_dim = 1  # Wavelength as input\n",
    "    hidden_dim = 64\n",
    "    latent_dim = 32\n",
    "    sequence_length = 17\n",
    "    \n",
    "    timegan = TimeGAN(input_dim, hidden_dim, latent_dim, device)\n",
    "    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    discriminator_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    generator_optimizer = optim.Adam(timegan.generator_net.parameters(), lr=0.00005, betas=(0.5, 0.999))\n",
    "    discriminator_optimizer = optim.Adam(timegan.discriminator_net.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for real_data in train_loader:\n",
    "            real_data = real_data.to(device)\n",
    "            wavelengths = real_data[:, :, 0:1]\n",
    "\n",
    "            # Discriminator Training (less frequent)\n",
    "            for _ in range(1):\n",
    "                noise = torch.randn(real_data.size(0), sequence_length, latent_dim).to(device)\n",
    "                generated_data = timegan.generator_net(noise, wavelengths)\n",
    "                \n",
    "                real_labels = torch.ones(real_data.size(0), 1).to(device) * 0.9\n",
    "                fake_labels = torch.zeros(real_data.size(0), 1).to(device) * 0.1\n",
    "                \n",
    "                real_data_noisy = real_data + 0.05 * torch.randn_like(real_data)\n",
    "                generated_data_noisy = generated_data + 0.05 * torch.randn_like(generated_data)\n",
    "\n",
    "                real_loss = discriminator_criterion(timegan.discriminator_net(real_data_noisy[:, :, 1:]), real_labels)\n",
    "                fake_loss = discriminator_criterion(timegan.discriminator_net(generated_data_noisy.detach()), fake_labels)\n",
    "\n",
    "                discriminator_loss = real_loss + fake_loss\n",
    "                \n",
    "                discriminator_optimizer.zero_grad()\n",
    "                discriminator_loss.backward()\n",
    "                discriminator_optimizer.step()\n",
    "\n",
    "            # Generator Training (more frequent)\n",
    "            for _ in range(2):\n",
    "                noise = torch.randn(real_data.size(0), sequence_length, latent_dim).to(device)\n",
    "                generated_data = timegan.generator_net(noise, wavelengths)\n",
    "                generator_loss = mse_loss(generated_data, real_data[:, :, 1:])\n",
    "                \n",
    "                generator_optimizer.zero_grad()\n",
    "                generator_loss.backward()\n",
    "                generator_optimizer.step()\n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Generator Loss: {generator_loss.item():.4f}, Discriminator Loss: {discriminator_loss.item():.4f}\")\n",
    "\n",
    "    torch.save({\n",
    "        'embedding_net': timegan.embedding_net.state_dict(),\n",
    "        'recovery_net': timegan.recovery_net.state_dict(),\n",
    "        'generator_net': timegan.generator_net.state_dict(),\n",
    "        'discriminator_net': timegan.discriminator_net.state_dict(),\n",
    "    }, 'new_timegan_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_fluxes_for_wavelengths(timegan, wavelengths, scaler, device, latent_dim=32):\n",
    "    wavelengths = wavelengths.to(device)\n",
    "    noise = torch.randn(1, wavelengths.size(0), latent_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_fluxes = timegan.generator_net(noise, wavelengths)\n",
    "    generated_fluxes = generated_fluxes.detach().cpu().numpy().reshape(wavelengths.size(0), -1)\n",
    "    generated_fluxes = scaler.inverse_transform(generated_fluxes)\n",
    "    return generated_fluxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_real_vs_generated(real_data, generated_data, scaler, num_samples=5):\n",
    "    \"\"\"\n",
    "    Plots a comparison between real and generated data.\n",
    "    \n",
    "    Parameters:\n",
    "    - real_data: torch.Tensor, the real data batch (shape: [batch_size, sequence_length, num_features])\n",
    "    - generated_data: torch.Tensor, the generated data batch (shape: [batch_size, sequence_length, num_features])\n",
    "    - scaler: fitted scaler for inverse normalization\n",
    "    - num_samples: int, the number of samples to plot (default is 5)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Move data to CPU if necessary\n",
    "    real_data = real_data.detach().cpu().numpy()\n",
    "    generated_data = generated_data.detach().cpu().numpy()\n",
    "    \n",
    "    # De-scale the generated data\n",
    "    batch_size, sequence_length, num_features = generated_data.shape\n",
    "    generated_data = generated_data.reshape(batch_size * sequence_length, num_features)\n",
    "    generated_data = scaler.inverse_transform(generated_data)\n",
    "    generated_data = generated_data.reshape(batch_size, sequence_length, num_features)\n",
    "    \n",
    "    # Ensure num_samples does not exceed batch size\n",
    "    num_samples = min(num_samples, real_data.shape[0])\n",
    "    \n",
    "    # Plot each sample\n",
    "    plt.figure(figsize=(10, 6 * num_samples))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(num_samples, 1, i + 1)\n",
    "        \n",
    "        # Plot real data vs generated data for each feature\n",
    "        plt.plot(real_data[i, :, 0], real_data[i, :, 2], label='Real', color='blue', alpha=0.7)\n",
    "        print(real_data[i, :, 0].shape, generated_data[i, :, 1].shape)\n",
    "        plt.scatter(real_data[i, :, 0], generated_data[i, :, 2], label='Generated', linestyle='dashed', color='red', alpha=0.7)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.title(f'Sample {i + 1} - Real vs Generated')\n",
    "        plt.xlabel('Wavelength')\n",
    "        plt.ylabel('Flux')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_flux_predictions(timegan, wavelengths, scaler, device, latent_dim=32):\n",
    "    \"\"\"\n",
    "    Generate new flux predictions using the trained TimeGAN model.\n",
    "    \n",
    "    Parameters:\n",
    "    - timegan: Trained TimeGAN model.\n",
    "    - wavelengths: Tensor of wavelengths for which to generate predictions (shape: [sequence_length, 1]).\n",
    "    - scaler: Fitted scaler used during training for normalization.\n",
    "    - device: Device (CPU/GPU) on which the model is running.\n",
    "    - latent_dim: Dimension of the noise vector used as input to the generator.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_fluxes: De-scaled predicted fluxes.\n",
    "    \"\"\"\n",
    "    # Ensure wavelengths are on the correct device\n",
    "    wavelengths = wavelengths.to(device).unsqueeze(0)  # Add batch dimension (1, seq_len, 1)\n",
    "    \n",
    "    # Generate a noise vector (batch_size=1 for a single prediction)\n",
    "    noise = torch.randn(1, wavelengths.size(1), latent_dim).to(device)\n",
    "    \n",
    "    # Use the generator to produce predicted fluxes\n",
    "    with torch.no_grad():\n",
    "        generated_fluxes = timegan.generator_net(noise, wavelengths)\n",
    "    \n",
    "    # De-scale the generated data\n",
    "    generated_fluxes = generated_fluxes.squeeze(0).detach().cpu().numpy()  # Remove batch dimension\n",
    "    generated_fluxes = scaler.inverse_transform(generated_fluxes)\n",
    "    \n",
    "    return generated_fluxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_galaxy_flux(timegan, wavelengths, scaler, device, latent_dim=32):\n",
    "    \"\"\"\n",
    "    Generate flux predictions for a single galaxy using the trained TimeGAN model.\n",
    "    \n",
    "    Parameters:\n",
    "    - timegan: Trained TimeGAN model.\n",
    "    - wavelengths: Tensor of wavelengths for which to generate predictions (shape: [sequence_length, 1]).\n",
    "    - scaler: Fitted scaler used during training for normalization.\n",
    "    - device: Device (CPU/GPU) on which the model is running.\n",
    "    - latent_dim: Dimension of the noise vector used as input to the generator.\n",
    "    \n",
    "    Returns:\n",
    "    - generated_fluxes: De-scaled predicted fluxes for a single galaxy.\n",
    "    \"\"\"\n",
    "    # Ensure wavelengths are on the correct device\n",
    "    wavelengths = wavelengths.to(device).unsqueeze(0)  # Add batch dimension (1, seq_len, 1)\n",
    "    \n",
    "    # Generate a noise vector (batch_size=1 for a single prediction)\n",
    "    noise = torch.randn(1, wavelengths.size(1), latent_dim).to(device)\n",
    "    \n",
    "    # Use the generator to produce predicted fluxes\n",
    "    with torch.no_grad():\n",
    "        generated_fluxes = timegan.generator_net(noise, wavelengths)\n",
    "    \n",
    "    # De-scale the generated data\n",
    "    generated_fluxes = generated_fluxes.squeeze(0).detach().cpu().numpy()  # Remove batch dimension\n",
    "    generated_fluxes = scaler.inverse_transform(generated_fluxes)  # Inverse scaling\n",
    "    \n",
    "    return generated_fluxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "def evaluate_model(timegan, real_loader, scaler, device, latent_dim=32):\n",
    "    mse_list = []\n",
    "    mae_list = []\n",
    "\n",
    "    timegan.generator_net.eval()  # Set model to evaluation mode\n",
    "\n",
    "    for real_data in real_loader:\n",
    "        real_data = real_data.to(device)\n",
    "        real_wavelengths = real_data[:, :, 0:1]  # Extract real wavelengths\n",
    "        real_fluxes = real_data[:, :, 1:]        # Extract real fluxes\n",
    "        \n",
    "        # Generate fluxes using the same wavelengths\n",
    "        noise = torch.randn(real_data.size(0), real_data.size(1), latent_dim).to(device)\n",
    "        generated_fluxes = timegan.generator_net(noise, real_wavelengths)\n",
    "        \n",
    "        # De-scale the generated and real fluxes\n",
    "        real_fluxes = scaler.inverse_transform(real_fluxes.detach().cpu().numpy().reshape(-1, real_fluxes.shape[-1]))\n",
    "        generated_fluxes = scaler.inverse_transform(generated_fluxes.detach().cpu().numpy().reshape(-1, generated_fluxes.shape[-1]))\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        mse = mean_squared_error(real_fluxes, generated_fluxes)\n",
    "        mae = mean_absolute_error(real_fluxes, generated_fluxes)\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        mae_list.append(mae)\n",
    "    \n",
    "    avg_mse = sum(mse_list) / len(mse_list)\n",
    "    avg_mae = sum(mae_list) / len(mae_list)\n",
    "\n",
    "    print(f\"Evaluation - Average MSE: {avg_mse:.4f}, Average MAE: {avg_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/100000], Generator Loss: 6.5780, Discriminator Loss: 1.3671\n",
      "Epoch [200/100000], Generator Loss: 4.7590, Discriminator Loss: 1.3152\n",
      "Epoch [300/100000], Generator Loss: 3.5269, Discriminator Loss: 1.2735\n",
      "Epoch [400/100000], Generator Loss: 2.8344, Discriminator Loss: 1.2425\n",
      "Epoch [500/100000], Generator Loss: 2.4932, Discriminator Loss: 1.2107\n",
      "Epoch [600/100000], Generator Loss: 2.3429, Discriminator Loss: 1.1831\n",
      "Epoch [700/100000], Generator Loss: 2.2701, Discriminator Loss: 1.1619\n",
      "Epoch [800/100000], Generator Loss: 1.9304, Discriminator Loss: 1.1451\n",
      "Epoch [900/100000], Generator Loss: 1.6025, Discriminator Loss: 1.1340\n",
      "Epoch [1000/100000], Generator Loss: 1.2553, Discriminator Loss: 1.1265\n",
      "Epoch [1100/100000], Generator Loss: 0.9477, Discriminator Loss: 1.1213\n",
      "Epoch [1200/100000], Generator Loss: 0.7357, Discriminator Loss: 1.3884\n",
      "Epoch [1300/100000], Generator Loss: 0.5953, Discriminator Loss: 1.3881\n",
      "Epoch [1400/100000], Generator Loss: 0.4814, Discriminator Loss: 1.3883\n",
      "Epoch [1500/100000], Generator Loss: 0.3772, Discriminator Loss: 1.3881\n",
      "Epoch [1600/100000], Generator Loss: 0.3305, Discriminator Loss: 1.3878\n",
      "Epoch [1700/100000], Generator Loss: 0.2880, Discriminator Loss: 1.3878\n",
      "Epoch [1800/100000], Generator Loss: 0.2453, Discriminator Loss: 1.3877\n",
      "Epoch [1900/100000], Generator Loss: 0.2277, Discriminator Loss: 1.3876\n",
      "Epoch [2000/100000], Generator Loss: 0.1827, Discriminator Loss: 1.3875\n",
      "Epoch [2100/100000], Generator Loss: 0.1529, Discriminator Loss: 1.3875\n",
      "Epoch [2200/100000], Generator Loss: 0.1435, Discriminator Loss: 1.3873\n",
      "Epoch [2300/100000], Generator Loss: 0.1365, Discriminator Loss: 1.3873\n",
      "Epoch [2400/100000], Generator Loss: 0.1341, Discriminator Loss: 1.3873\n",
      "Epoch [2500/100000], Generator Loss: 0.1657, Discriminator Loss: 1.3871\n",
      "Epoch [2600/100000], Generator Loss: 0.1222, Discriminator Loss: 1.3871\n",
      "Epoch [2700/100000], Generator Loss: 0.1028, Discriminator Loss: 1.3870\n",
      "Epoch [2800/100000], Generator Loss: 0.1049, Discriminator Loss: 1.3869\n",
      "Epoch [2900/100000], Generator Loss: 0.0994, Discriminator Loss: 1.3869\n",
      "Epoch [3000/100000], Generator Loss: 0.0970, Discriminator Loss: 1.3869\n",
      "Epoch [3100/100000], Generator Loss: 0.1100, Discriminator Loss: 1.3868\n",
      "Epoch [3200/100000], Generator Loss: 0.0791, Discriminator Loss: 1.3868\n",
      "Epoch [3300/100000], Generator Loss: 0.0754, Discriminator Loss: 1.3867\n",
      "Epoch [3400/100000], Generator Loss: 0.0811, Discriminator Loss: 1.3867\n",
      "Epoch [3500/100000], Generator Loss: 0.0689, Discriminator Loss: 1.3867\n",
      "Epoch [3600/100000], Generator Loss: 0.0599, Discriminator Loss: 1.3866\n",
      "Epoch [3700/100000], Generator Loss: 0.0829, Discriminator Loss: 1.3866\n",
      "Epoch [3800/100000], Generator Loss: 0.0808, Discriminator Loss: 1.3866\n",
      "Epoch [3900/100000], Generator Loss: 0.0564, Discriminator Loss: 1.3866\n",
      "Epoch [4000/100000], Generator Loss: 0.0529, Discriminator Loss: 1.3866\n",
      "Epoch [4100/100000], Generator Loss: 0.0526, Discriminator Loss: 1.3865\n",
      "Epoch [4200/100000], Generator Loss: 0.0506, Discriminator Loss: 1.3866\n",
      "Epoch [4300/100000], Generator Loss: 0.0405, Discriminator Loss: 1.3865\n",
      "Epoch [4400/100000], Generator Loss: 0.0419, Discriminator Loss: 1.3865\n",
      "Epoch [4500/100000], Generator Loss: 0.0603, Discriminator Loss: 1.3865\n",
      "Epoch [4600/100000], Generator Loss: 0.0499, Discriminator Loss: 1.3865\n",
      "Epoch [4700/100000], Generator Loss: 0.0391, Discriminator Loss: 1.3864\n",
      "Epoch [4800/100000], Generator Loss: 0.0376, Discriminator Loss: 1.3864\n",
      "Epoch [4900/100000], Generator Loss: 0.0315, Discriminator Loss: 1.3864\n",
      "Epoch [5000/100000], Generator Loss: 0.0350, Discriminator Loss: 1.3864\n",
      "Epoch [5100/100000], Generator Loss: 0.0396, Discriminator Loss: 1.3864\n",
      "Epoch [5200/100000], Generator Loss: 0.0385, Discriminator Loss: 1.3864\n",
      "Epoch [5300/100000], Generator Loss: 0.0395, Discriminator Loss: 1.3864\n",
      "Epoch [5400/100000], Generator Loss: 0.0306, Discriminator Loss: 1.3864\n",
      "Epoch [5500/100000], Generator Loss: 0.0381, Discriminator Loss: 1.3864\n",
      "Epoch [5600/100000], Generator Loss: 0.0352, Discriminator Loss: 1.3864\n",
      "Epoch [5700/100000], Generator Loss: 0.0408, Discriminator Loss: 1.3864\n",
      "Epoch [5800/100000], Generator Loss: 0.0304, Discriminator Loss: 1.3864\n",
      "Epoch [5900/100000], Generator Loss: 0.0292, Discriminator Loss: 1.3864\n",
      "Epoch [6000/100000], Generator Loss: 0.0277, Discriminator Loss: 1.3864\n",
      "Epoch [6100/100000], Generator Loss: 0.0387, Discriminator Loss: 1.3864\n",
      "Epoch [6200/100000], Generator Loss: 0.0305, Discriminator Loss: 1.3863\n",
      "Epoch [6300/100000], Generator Loss: 0.0277, Discriminator Loss: 1.3863\n",
      "Epoch [6400/100000], Generator Loss: 0.0231, Discriminator Loss: 1.3863\n",
      "Epoch [6500/100000], Generator Loss: 0.0277, Discriminator Loss: 1.3863\n",
      "Epoch [6600/100000], Generator Loss: 0.0268, Discriminator Loss: 1.3863\n",
      "Epoch [6700/100000], Generator Loss: 0.0262, Discriminator Loss: 1.3863\n",
      "Epoch [6800/100000], Generator Loss: 0.0224, Discriminator Loss: 1.3863\n",
      "Epoch [6900/100000], Generator Loss: 0.0188, Discriminator Loss: 1.3863\n",
      "Epoch [7000/100000], Generator Loss: 0.0286, Discriminator Loss: 1.3863\n",
      "Epoch [7100/100000], Generator Loss: 0.0208, Discriminator Loss: 1.3863\n",
      "Epoch [7200/100000], Generator Loss: 0.0251, Discriminator Loss: 1.3863\n",
      "Epoch [7300/100000], Generator Loss: 0.0184, Discriminator Loss: 1.3863\n",
      "Epoch [7400/100000], Generator Loss: 0.0222, Discriminator Loss: 1.3863\n",
      "Epoch [7500/100000], Generator Loss: 0.0180, Discriminator Loss: 1.3863\n",
      "Epoch [7600/100000], Generator Loss: 0.0289, Discriminator Loss: 1.3863\n",
      "Epoch [7700/100000], Generator Loss: 0.0217, Discriminator Loss: 1.3863\n",
      "Epoch [7800/100000], Generator Loss: 0.0177, Discriminator Loss: 1.3863\n",
      "Epoch [7900/100000], Generator Loss: 0.0256, Discriminator Loss: 1.3863\n",
      "Epoch [8000/100000], Generator Loss: 0.0214, Discriminator Loss: 1.3863\n",
      "Epoch [8100/100000], Generator Loss: 0.0241, Discriminator Loss: 1.3863\n",
      "Epoch [8200/100000], Generator Loss: 0.0179, Discriminator Loss: 1.3863\n",
      "Epoch [8300/100000], Generator Loss: 0.0150, Discriminator Loss: 1.3863\n",
      "Epoch [8400/100000], Generator Loss: 0.0226, Discriminator Loss: 1.3863\n",
      "Epoch [8500/100000], Generator Loss: 0.0147, Discriminator Loss: 1.3863\n",
      "Epoch [8600/100000], Generator Loss: 0.0183, Discriminator Loss: 1.3863\n",
      "Epoch [8700/100000], Generator Loss: 0.0206, Discriminator Loss: 1.3863\n",
      "Epoch [8800/100000], Generator Loss: 0.0204, Discriminator Loss: 1.3863\n",
      "Epoch [8900/100000], Generator Loss: 0.0175, Discriminator Loss: 1.3863\n",
      "Epoch [9000/100000], Generator Loss: 0.0145, Discriminator Loss: 1.3863\n",
      "Epoch [9100/100000], Generator Loss: 0.0176, Discriminator Loss: 1.3863\n",
      "Epoch [9200/100000], Generator Loss: 0.0260, Discriminator Loss: 1.3863\n",
      "Epoch [9300/100000], Generator Loss: 0.0143, Discriminator Loss: 1.3863\n",
      "Epoch [9400/100000], Generator Loss: 0.0132, Discriminator Loss: 1.3863\n",
      "Epoch [9500/100000], Generator Loss: 0.0164, Discriminator Loss: 1.3863\n",
      "Epoch [9600/100000], Generator Loss: 0.0170, Discriminator Loss: 1.3863\n",
      "Epoch [9700/100000], Generator Loss: 0.0170, Discriminator Loss: 1.3863\n",
      "Epoch [9800/100000], Generator Loss: 0.0175, Discriminator Loss: 1.3863\n",
      "Epoch [9900/100000], Generator Loss: 0.0167, Discriminator Loss: 1.3863\n",
      "Epoch [10000/100000], Generator Loss: 0.0113, Discriminator Loss: 1.3863\n",
      "Epoch [10100/100000], Generator Loss: 0.0120, Discriminator Loss: 1.3863\n",
      "Epoch [10200/100000], Generator Loss: 0.0184, Discriminator Loss: 1.3863\n",
      "Epoch [10300/100000], Generator Loss: 0.0121, Discriminator Loss: 1.3863\n",
      "Epoch [10400/100000], Generator Loss: 0.0124, Discriminator Loss: 1.3863\n",
      "Epoch [10500/100000], Generator Loss: 0.0135, Discriminator Loss: 1.3863\n",
      "Epoch [10600/100000], Generator Loss: 0.0173, Discriminator Loss: 1.3863\n",
      "Epoch [10700/100000], Generator Loss: 0.0127, Discriminator Loss: 1.3863\n",
      "Epoch [10800/100000], Generator Loss: 0.0127, Discriminator Loss: 1.3863\n",
      "Epoch [10900/100000], Generator Loss: 0.0127, Discriminator Loss: 1.3863\n",
      "Epoch [11000/100000], Generator Loss: 0.0194, Discriminator Loss: 1.3863\n",
      "Epoch [11100/100000], Generator Loss: 0.0161, Discriminator Loss: 1.3863\n",
      "Epoch [11200/100000], Generator Loss: 0.0130, Discriminator Loss: 1.3863\n",
      "Epoch [11300/100000], Generator Loss: 0.0113, Discriminator Loss: 1.3863\n",
      "Epoch [11400/100000], Generator Loss: 0.0105, Discriminator Loss: 1.3863\n",
      "Epoch [11500/100000], Generator Loss: 0.0113, Discriminator Loss: 1.3863\n",
      "Epoch [11600/100000], Generator Loss: 0.0162, Discriminator Loss: 1.3863\n",
      "Epoch [11700/100000], Generator Loss: 0.0132, Discriminator Loss: 1.3863\n",
      "Epoch [11800/100000], Generator Loss: 0.0145, Discriminator Loss: 1.3863\n",
      "Epoch [11900/100000], Generator Loss: 0.0170, Discriminator Loss: 1.3863\n",
      "Epoch [12000/100000], Generator Loss: 0.0120, Discriminator Loss: 1.3863\n",
      "Epoch [12100/100000], Generator Loss: 0.0073, Discriminator Loss: 1.3863\n",
      "Epoch [12200/100000], Generator Loss: 0.0104, Discriminator Loss: 1.3863\n",
      "Epoch [12300/100000], Generator Loss: 0.0073, Discriminator Loss: 1.3863\n",
      "Epoch [12400/100000], Generator Loss: 0.0094, Discriminator Loss: 1.3863\n",
      "Epoch [12500/100000], Generator Loss: 0.0124, Discriminator Loss: 1.3863\n",
      "Epoch [12600/100000], Generator Loss: 0.0136, Discriminator Loss: 1.3863\n",
      "Epoch [12700/100000], Generator Loss: 0.0084, Discriminator Loss: 1.3863\n",
      "Epoch [12800/100000], Generator Loss: 0.0137, Discriminator Loss: 1.3863\n",
      "Epoch [12900/100000], Generator Loss: 0.0102, Discriminator Loss: 1.3863\n",
      "Epoch [13000/100000], Generator Loss: 0.0103, Discriminator Loss: 1.3863\n",
      "Epoch [13100/100000], Generator Loss: 0.0109, Discriminator Loss: 1.3863\n",
      "Epoch [13200/100000], Generator Loss: 0.0086, Discriminator Loss: 1.3863\n",
      "Epoch [13300/100000], Generator Loss: 0.0083, Discriminator Loss: 1.3863\n",
      "Epoch [13400/100000], Generator Loss: 0.0109, Discriminator Loss: 1.3863\n",
      "Epoch [13500/100000], Generator Loss: 0.0100, Discriminator Loss: 1.3863\n",
      "Epoch [13600/100000], Generator Loss: 0.0119, Discriminator Loss: 1.3863\n",
      "Epoch [13700/100000], Generator Loss: 0.0112, Discriminator Loss: 1.3863\n",
      "Epoch [13800/100000], Generator Loss: 0.0078, Discriminator Loss: 1.3863\n",
      "Epoch [13900/100000], Generator Loss: 0.0076, Discriminator Loss: 1.3863\n",
      "Epoch [14000/100000], Generator Loss: 0.0089, Discriminator Loss: 1.3863\n",
      "Epoch [14100/100000], Generator Loss: 0.0084, Discriminator Loss: 1.3863\n",
      "Epoch [14200/100000], Generator Loss: 0.0100, Discriminator Loss: 1.3863\n",
      "Epoch [14300/100000], Generator Loss: 0.0109, Discriminator Loss: 1.3863\n",
      "Epoch [14400/100000], Generator Loss: 0.0068, Discriminator Loss: 1.3863\n",
      "Epoch [14500/100000], Generator Loss: 0.0080, Discriminator Loss: 1.3863\n",
      "Epoch [14600/100000], Generator Loss: 0.0083, Discriminator Loss: 1.3863\n",
      "Epoch [14700/100000], Generator Loss: 0.0067, Discriminator Loss: 1.3863\n",
      "Epoch [14800/100000], Generator Loss: 0.0105, Discriminator Loss: 1.3863\n",
      "Epoch [14900/100000], Generator Loss: 0.0072, Discriminator Loss: 1.3863\n",
      "Epoch [15000/100000], Generator Loss: 0.0095, Discriminator Loss: 1.3863\n",
      "Epoch [15100/100000], Generator Loss: 0.0075, Discriminator Loss: 1.3863\n",
      "Epoch [15200/100000], Generator Loss: 0.0086, Discriminator Loss: 1.3863\n",
      "Epoch [15300/100000], Generator Loss: 0.0092, Discriminator Loss: 1.3863\n",
      "Epoch [15400/100000], Generator Loss: 0.0076, Discriminator Loss: 1.3863\n",
      "Epoch [15500/100000], Generator Loss: 0.0084, Discriminator Loss: 1.3863\n",
      "Epoch [15600/100000], Generator Loss: 0.0083, Discriminator Loss: 1.3863\n",
      "Epoch [15700/100000], Generator Loss: 0.0087, Discriminator Loss: 1.3863\n",
      "Epoch [15800/100000], Generator Loss: 0.0089, Discriminator Loss: 1.3863\n",
      "Epoch [15900/100000], Generator Loss: 0.0068, Discriminator Loss: 1.3863\n",
      "Epoch [16000/100000], Generator Loss: 0.0086, Discriminator Loss: 1.3863\n",
      "Epoch [16100/100000], Generator Loss: 0.0058, Discriminator Loss: 1.3863\n",
      "Epoch [16200/100000], Generator Loss: 0.0057, Discriminator Loss: 1.3863\n",
      "Epoch [16300/100000], Generator Loss: 0.0096, Discriminator Loss: 1.3863\n",
      "Epoch [16400/100000], Generator Loss: 0.0072, Discriminator Loss: 1.3863\n",
      "Epoch [16500/100000], Generator Loss: 0.0062, Discriminator Loss: 1.3863\n",
      "Epoch [16600/100000], Generator Loss: 0.0062, Discriminator Loss: 1.3863\n",
      "Epoch [16700/100000], Generator Loss: 0.0072, Discriminator Loss: 1.3863\n",
      "Epoch [16800/100000], Generator Loss: 0.0122, Discriminator Loss: 1.3863\n",
      "Epoch [16900/100000], Generator Loss: 0.0047, Discriminator Loss: 1.3863\n",
      "Epoch [17000/100000], Generator Loss: 0.0092, Discriminator Loss: 1.3863\n",
      "Epoch [17100/100000], Generator Loss: 0.0071, Discriminator Loss: 1.3863\n",
      "Epoch [17200/100000], Generator Loss: 0.0075, Discriminator Loss: 1.3863\n",
      "Epoch [17300/100000], Generator Loss: 0.0147, Discriminator Loss: 1.3863\n",
      "Epoch [17400/100000], Generator Loss: 0.0077, Discriminator Loss: 1.3863\n",
      "Epoch [17500/100000], Generator Loss: 0.0080, Discriminator Loss: 1.3863\n",
      "Epoch [17600/100000], Generator Loss: 0.0072, Discriminator Loss: 1.3863\n",
      "Epoch [17700/100000], Generator Loss: 0.0086, Discriminator Loss: 1.3863\n",
      "Epoch [17800/100000], Generator Loss: 0.0080, Discriminator Loss: 1.3863\n",
      "Epoch [17900/100000], Generator Loss: 0.0063, Discriminator Loss: 1.3863\n",
      "Epoch [18000/100000], Generator Loss: 0.0060, Discriminator Loss: 1.3863\n",
      "Epoch [18100/100000], Generator Loss: 0.0044, Discriminator Loss: 1.3863\n",
      "Epoch [18200/100000], Generator Loss: 0.0078, Discriminator Loss: 1.3863\n",
      "Epoch [18300/100000], Generator Loss: 0.0104, Discriminator Loss: 1.3863\n",
      "Epoch [18400/100000], Generator Loss: 0.0054, Discriminator Loss: 1.3863\n",
      "Epoch [18500/100000], Generator Loss: 0.0048, Discriminator Loss: 1.3863\n",
      "Epoch [18600/100000], Generator Loss: 0.0071, Discriminator Loss: 1.3863\n",
      "Epoch [18700/100000], Generator Loss: 0.0081, Discriminator Loss: 1.3863\n",
      "Epoch [18800/100000], Generator Loss: 0.0095, Discriminator Loss: 1.3863\n",
      "Epoch [18900/100000], Generator Loss: 0.0144, Discriminator Loss: 1.3863\n",
      "Epoch [19000/100000], Generator Loss: 0.0051, Discriminator Loss: 1.3863\n",
      "Epoch [19100/100000], Generator Loss: 0.0059, Discriminator Loss: 1.3863\n",
      "Epoch [19200/100000], Generator Loss: 0.0063, Discriminator Loss: 1.3863\n",
      "Epoch [19300/100000], Generator Loss: 0.0062, Discriminator Loss: 1.3863\n",
      "Epoch [19400/100000], Generator Loss: 0.0071, Discriminator Loss: 1.3863\n",
      "Epoch [19500/100000], Generator Loss: 0.0046, Discriminator Loss: 1.3863\n",
      "Epoch [19600/100000], Generator Loss: 0.0099, Discriminator Loss: 1.3863\n",
      "Epoch [19700/100000], Generator Loss: 0.0070, Discriminator Loss: 1.3863\n",
      "Epoch [19800/100000], Generator Loss: 0.0065, Discriminator Loss: 1.3863\n",
      "Epoch [19900/100000], Generator Loss: 0.0089, Discriminator Loss: 1.3863\n",
      "Epoch [20000/100000], Generator Loss: 0.0071, Discriminator Loss: 1.3863\n",
      "Epoch [20100/100000], Generator Loss: 0.0047, Discriminator Loss: 1.3863\n",
      "Epoch [20200/100000], Generator Loss: 0.0053, Discriminator Loss: 1.3863\n",
      "Epoch [20300/100000], Generator Loss: 0.0045, Discriminator Loss: 1.3863\n",
      "Epoch [20400/100000], Generator Loss: 0.0043, Discriminator Loss: 1.3863\n",
      "Epoch [20500/100000], Generator Loss: 0.0050, Discriminator Loss: 1.3863\n",
      "Epoch [20600/100000], Generator Loss: 0.0073, Discriminator Loss: 1.3863\n",
      "Epoch [20700/100000], Generator Loss: 0.0044, Discriminator Loss: 1.3863\n",
      "Epoch [20800/100000], Generator Loss: 0.0056, Discriminator Loss: 1.3863\n",
      "Epoch [20900/100000], Generator Loss: 0.0055, Discriminator Loss: 1.3863\n",
      "Epoch [21000/100000], Generator Loss: 0.0048, Discriminator Loss: 1.3863\n",
      "Epoch [21100/100000], Generator Loss: 0.0059, Discriminator Loss: 1.3863\n",
      "Epoch [21200/100000], Generator Loss: 0.0058, Discriminator Loss: 1.3863\n",
      "Epoch [21300/100000], Generator Loss: 0.0046, Discriminator Loss: 1.3863\n",
      "Epoch [21400/100000], Generator Loss: 0.0035, Discriminator Loss: 1.3863\n",
      "Epoch [21500/100000], Generator Loss: 0.0079, Discriminator Loss: 1.3863\n",
      "Epoch [21600/100000], Generator Loss: 0.0042, Discriminator Loss: 1.3863\n",
      "Epoch [21700/100000], Generator Loss: 0.0065, Discriminator Loss: 1.3863\n",
      "Epoch [21800/100000], Generator Loss: 0.0044, Discriminator Loss: 1.3863\n",
      "Epoch [21900/100000], Generator Loss: 0.0043, Discriminator Loss: 1.3863\n",
      "Epoch [22000/100000], Generator Loss: 0.0059, Discriminator Loss: 1.3863\n",
      "Epoch [22100/100000], Generator Loss: 0.0049, Discriminator Loss: 1.3863\n",
      "Epoch [22200/100000], Generator Loss: 0.0043, Discriminator Loss: 1.3863\n",
      "Epoch [22300/100000], Generator Loss: 0.0065, Discriminator Loss: 1.3863\n",
      "Epoch [22400/100000], Generator Loss: 0.0068, Discriminator Loss: 1.3863\n",
      "Epoch [22500/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [22600/100000], Generator Loss: 0.0046, Discriminator Loss: 1.3863\n",
      "Epoch [22700/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [22800/100000], Generator Loss: 0.0052, Discriminator Loss: 1.3863\n",
      "Epoch [22900/100000], Generator Loss: 0.0036, Discriminator Loss: 1.3863\n",
      "Epoch [23000/100000], Generator Loss: 0.0069, Discriminator Loss: 1.3863\n",
      "Epoch [23100/100000], Generator Loss: 0.0083, Discriminator Loss: 1.3863\n",
      "Epoch [23200/100000], Generator Loss: 0.0045, Discriminator Loss: 1.3863\n",
      "Epoch [23300/100000], Generator Loss: 0.0047, Discriminator Loss: 1.3863\n",
      "Epoch [23400/100000], Generator Loss: 0.0066, Discriminator Loss: 1.3863\n",
      "Epoch [23500/100000], Generator Loss: 0.0042, Discriminator Loss: 1.3863\n",
      "Epoch [23600/100000], Generator Loss: 0.0072, Discriminator Loss: 1.3863\n",
      "Epoch [23700/100000], Generator Loss: 0.0146, Discriminator Loss: 1.3863\n",
      "Epoch [23800/100000], Generator Loss: 0.0077, Discriminator Loss: 1.3863\n",
      "Epoch [23900/100000], Generator Loss: 0.0050, Discriminator Loss: 1.3863\n",
      "Epoch [24000/100000], Generator Loss: 0.0041, Discriminator Loss: 1.3863\n",
      "Epoch [24100/100000], Generator Loss: 0.0033, Discriminator Loss: 1.3863\n",
      "Epoch [24200/100000], Generator Loss: 0.0089, Discriminator Loss: 1.3863\n",
      "Epoch [24300/100000], Generator Loss: 0.0035, Discriminator Loss: 1.3863\n",
      "Epoch [24400/100000], Generator Loss: 0.0044, Discriminator Loss: 1.3863\n",
      "Epoch [24500/100000], Generator Loss: 0.0049, Discriminator Loss: 1.3863\n",
      "Epoch [24600/100000], Generator Loss: 0.0037, Discriminator Loss: 1.3863\n",
      "Epoch [24700/100000], Generator Loss: 0.0040, Discriminator Loss: 1.3863\n",
      "Epoch [24800/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [24900/100000], Generator Loss: 0.0054, Discriminator Loss: 1.3863\n",
      "Epoch [25000/100000], Generator Loss: 0.0067, Discriminator Loss: 1.3863\n",
      "Epoch [25100/100000], Generator Loss: 0.0045, Discriminator Loss: 1.3863\n",
      "Epoch [25200/100000], Generator Loss: 0.0037, Discriminator Loss: 1.3863\n",
      "Epoch [25300/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [25400/100000], Generator Loss: 0.0038, Discriminator Loss: 1.3863\n",
      "Epoch [25500/100000], Generator Loss: 0.0056, Discriminator Loss: 1.3863\n",
      "Epoch [25600/100000], Generator Loss: 0.0035, Discriminator Loss: 1.3863\n",
      "Epoch [25700/100000], Generator Loss: 0.0033, Discriminator Loss: 1.3863\n",
      "Epoch [25800/100000], Generator Loss: 0.0035, Discriminator Loss: 1.3863\n",
      "Epoch [25900/100000], Generator Loss: 0.0043, Discriminator Loss: 1.3863\n",
      "Epoch [26000/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [26100/100000], Generator Loss: 0.0048, Discriminator Loss: 1.3863\n",
      "Epoch [26200/100000], Generator Loss: 0.0063, Discriminator Loss: 1.3863\n",
      "Epoch [26300/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [26400/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [26500/100000], Generator Loss: 0.0043, Discriminator Loss: 1.3863\n",
      "Epoch [26600/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [26700/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [26800/100000], Generator Loss: 0.0038, Discriminator Loss: 1.3863\n",
      "Epoch [26900/100000], Generator Loss: 0.0041, Discriminator Loss: 1.3863\n",
      "Epoch [27000/100000], Generator Loss: 0.0036, Discriminator Loss: 1.3863\n",
      "Epoch [27100/100000], Generator Loss: 0.0040, Discriminator Loss: 1.3863\n",
      "Epoch [27200/100000], Generator Loss: 0.0067, Discriminator Loss: 1.3863\n",
      "Epoch [27300/100000], Generator Loss: 0.0062, Discriminator Loss: 1.3863\n",
      "Epoch [27400/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [27500/100000], Generator Loss: 0.0029, Discriminator Loss: 1.3863\n",
      "Epoch [27600/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [27700/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [27800/100000], Generator Loss: 0.0062, Discriminator Loss: 1.3863\n",
      "Epoch [27900/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [28000/100000], Generator Loss: 0.0038, Discriminator Loss: 1.3863\n",
      "Epoch [28100/100000], Generator Loss: 0.0051, Discriminator Loss: 1.3863\n",
      "Epoch [28200/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [28300/100000], Generator Loss: 0.0035, Discriminator Loss: 1.3863\n",
      "Epoch [28400/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [28500/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [28600/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [28700/100000], Generator Loss: 0.0098, Discriminator Loss: 1.3863\n",
      "Epoch [28800/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [28900/100000], Generator Loss: 0.0041, Discriminator Loss: 1.3863\n",
      "Epoch [29000/100000], Generator Loss: 0.0038, Discriminator Loss: 1.3863\n",
      "Epoch [29100/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [29200/100000], Generator Loss: 0.0049, Discriminator Loss: 1.3863\n",
      "Epoch [29300/100000], Generator Loss: 0.0032, Discriminator Loss: 1.3863\n",
      "Epoch [29400/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [29500/100000], Generator Loss: 0.0043, Discriminator Loss: 1.3863\n",
      "Epoch [29600/100000], Generator Loss: 0.0045, Discriminator Loss: 1.3863\n",
      "Epoch [29700/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [29800/100000], Generator Loss: 0.0029, Discriminator Loss: 1.3863\n",
      "Epoch [29900/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [30000/100000], Generator Loss: 0.0049, Discriminator Loss: 1.3863\n",
      "Epoch [30100/100000], Generator Loss: 0.0037, Discriminator Loss: 1.3863\n",
      "Epoch [30200/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [30300/100000], Generator Loss: 0.0041, Discriminator Loss: 1.3863\n",
      "Epoch [30400/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [30500/100000], Generator Loss: 0.0031, Discriminator Loss: 1.3863\n",
      "Epoch [30600/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [30700/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [30800/100000], Generator Loss: 0.0050, Discriminator Loss: 1.3863\n",
      "Epoch [30900/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [31000/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [31100/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [31200/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [31300/100000], Generator Loss: 0.0029, Discriminator Loss: 1.3863\n",
      "Epoch [31400/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [31500/100000], Generator Loss: 0.0029, Discriminator Loss: 1.3863\n",
      "Epoch [31600/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [31700/100000], Generator Loss: 0.0031, Discriminator Loss: 1.3863\n",
      "Epoch [31800/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [31900/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [32000/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [32100/100000], Generator Loss: 0.0074, Discriminator Loss: 1.3863\n",
      "Epoch [32200/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [32300/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [32400/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [32500/100000], Generator Loss: 0.0041, Discriminator Loss: 1.3863\n",
      "Epoch [32600/100000], Generator Loss: 0.0054, Discriminator Loss: 1.3863\n",
      "Epoch [32700/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [32800/100000], Generator Loss: 0.0040, Discriminator Loss: 1.3863\n",
      "Epoch [32900/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [33000/100000], Generator Loss: 0.0031, Discriminator Loss: 1.3863\n",
      "Epoch [33100/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [33200/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [33300/100000], Generator Loss: 0.0046, Discriminator Loss: 1.3863\n",
      "Epoch [33400/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [33500/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [33600/100000], Generator Loss: 0.0031, Discriminator Loss: 1.3863\n",
      "Epoch [33700/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [33800/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [33900/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [34000/100000], Generator Loss: 0.0042, Discriminator Loss: 1.3863\n",
      "Epoch [34100/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [34200/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [34300/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [34400/100000], Generator Loss: 0.0041, Discriminator Loss: 1.3863\n",
      "Epoch [34500/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [34600/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [34700/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [34800/100000], Generator Loss: 0.0099, Discriminator Loss: 1.3863\n",
      "Epoch [34900/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [35000/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [35100/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [35200/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [35300/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [35400/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [35500/100000], Generator Loss: 0.0037, Discriminator Loss: 1.3863\n",
      "Epoch [35600/100000], Generator Loss: 0.0029, Discriminator Loss: 1.3863\n",
      "Epoch [35700/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [35800/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [35900/100000], Generator Loss: 0.0036, Discriminator Loss: 1.3863\n",
      "Epoch [36000/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [36100/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [36200/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [36300/100000], Generator Loss: 0.0116, Discriminator Loss: 1.3863\n",
      "Epoch [36400/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [36500/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [36600/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [36700/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [36800/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [36900/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [37000/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [37100/100000], Generator Loss: 0.0033, Discriminator Loss: 1.3863\n",
      "Epoch [37200/100000], Generator Loss: 0.0092, Discriminator Loss: 1.3863\n",
      "Epoch [37300/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [37400/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [37500/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [37600/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [37700/100000], Generator Loss: 0.0029, Discriminator Loss: 1.3863\n",
      "Epoch [37800/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [37900/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [38000/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [38100/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [38200/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [38300/100000], Generator Loss: 0.0032, Discriminator Loss: 1.3863\n",
      "Epoch [38400/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [38500/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [38600/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [38700/100000], Generator Loss: 0.0033, Discriminator Loss: 1.3863\n",
      "Epoch [38800/100000], Generator Loss: 0.0036, Discriminator Loss: 1.3863\n",
      "Epoch [38900/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [39000/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [39100/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [39200/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [39300/100000], Generator Loss: 0.0032, Discriminator Loss: 1.3863\n",
      "Epoch [39400/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [39500/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [39600/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [39700/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [39800/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [39900/100000], Generator Loss: 0.0072, Discriminator Loss: 1.3863\n",
      "Epoch [40000/100000], Generator Loss: 0.0036, Discriminator Loss: 1.3863\n",
      "Epoch [40100/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [40200/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [40300/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [40400/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [40500/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [40600/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [40700/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [40800/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [40900/100000], Generator Loss: 0.0049, Discriminator Loss: 1.3863\n",
      "Epoch [41000/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [41100/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [41200/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [41300/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [41400/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [41500/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [41600/100000], Generator Loss: 0.0029, Discriminator Loss: 1.3863\n",
      "Epoch [41700/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [41800/100000], Generator Loss: 0.0041, Discriminator Loss: 1.3863\n",
      "Epoch [41900/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [42000/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [42100/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [42200/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [42300/100000], Generator Loss: 0.0050, Discriminator Loss: 1.3863\n",
      "Epoch [42400/100000], Generator Loss: 0.0044, Discriminator Loss: 1.3863\n",
      "Epoch [42500/100000], Generator Loss: 0.0033, Discriminator Loss: 1.3863\n",
      "Epoch [42600/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [42700/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [42800/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [42900/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [43000/100000], Generator Loss: 0.0055, Discriminator Loss: 1.3863\n",
      "Epoch [43100/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [43200/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [43300/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [43400/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [43500/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [43600/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [43700/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [43800/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [43900/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [44000/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [44100/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [44200/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [44300/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [44400/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [44500/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [44600/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [44700/100000], Generator Loss: 0.0032, Discriminator Loss: 1.3863\n",
      "Epoch [44800/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [44900/100000], Generator Loss: 0.0032, Discriminator Loss: 1.3863\n",
      "Epoch [45000/100000], Generator Loss: 0.0032, Discriminator Loss: 1.3863\n",
      "Epoch [45100/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [45200/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [45300/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [45400/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [45500/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [45600/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [45700/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [45800/100000], Generator Loss: 0.0049, Discriminator Loss: 1.3863\n",
      "Epoch [45900/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [46000/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [46100/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [46200/100000], Generator Loss: 0.0071, Discriminator Loss: 1.3863\n",
      "Epoch [46300/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [46400/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [46500/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [46600/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [46700/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [46800/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [46900/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [47000/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [47100/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [47200/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [47300/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [47400/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [47500/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [47600/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [47700/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [47800/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [47900/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [48000/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [48100/100000], Generator Loss: 0.0048, Discriminator Loss: 1.3863\n",
      "Epoch [48200/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [48300/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [48400/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [48500/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [48600/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [48700/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [48800/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [48900/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [49000/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [49100/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [49200/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [49300/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [49400/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [49500/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [49600/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [49700/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [49800/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [49900/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [50000/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [50100/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [50200/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [50300/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [50400/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [50500/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [50600/100000], Generator Loss: 0.0026, Discriminator Loss: 1.3863\n",
      "Epoch [50700/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [50800/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [50900/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [51000/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [51100/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [51200/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [51300/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [51400/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [51500/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [51600/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [51700/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [51800/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [51900/100000], Generator Loss: 0.0059, Discriminator Loss: 1.3863\n",
      "Epoch [52000/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [52100/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [52200/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [52300/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [52400/100000], Generator Loss: 0.0065, Discriminator Loss: 1.3863\n",
      "Epoch [52500/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [52600/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [52700/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [52800/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [52900/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [53000/100000], Generator Loss: 0.0034, Discriminator Loss: 1.3863\n",
      "Epoch [53100/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [53200/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [53300/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [53400/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [53500/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [53600/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [53700/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [53800/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [53900/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [54000/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [54100/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [54200/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [54300/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [54400/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [54500/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [54600/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [54700/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [54800/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [54900/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [55000/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [55100/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [55200/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [55300/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [55400/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [55500/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [55600/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [55700/100000], Generator Loss: 0.0038, Discriminator Loss: 1.3863\n",
      "Epoch [55800/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [55900/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [56000/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [56100/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [56200/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [56300/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [56400/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [56500/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [56600/100000], Generator Loss: 0.0148, Discriminator Loss: 1.3863\n",
      "Epoch [56700/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [56800/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [56900/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [57000/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [57100/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [57200/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [57300/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [57400/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [57500/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [57600/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [57700/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [57800/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [57900/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [58000/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [58100/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [58200/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [58300/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [58400/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [58500/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [58600/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [58700/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [58800/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [58900/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [59000/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [59100/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [59200/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [59300/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [59400/100000], Generator Loss: 0.0052, Discriminator Loss: 1.3863\n",
      "Epoch [59500/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [59600/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [59700/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [59800/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [59900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [60000/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [60100/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [60200/100000], Generator Loss: 0.0045, Discriminator Loss: 1.3863\n",
      "Epoch [60300/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [60400/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [60500/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [60600/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [60700/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [60800/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [60900/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [61000/100000], Generator Loss: 0.0383, Discriminator Loss: 1.3863\n",
      "Epoch [61100/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [61200/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [61300/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [61400/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [61500/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [61600/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [61700/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [61800/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [61900/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [62000/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [62100/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [62200/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [62300/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [62400/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [62500/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [62600/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [62700/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [62800/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [62900/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [63000/100000], Generator Loss: 0.0030, Discriminator Loss: 1.3863\n",
      "Epoch [63100/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [63200/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [63300/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [63400/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [63500/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [63600/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [63700/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [63800/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [63900/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [64000/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [64100/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [64200/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [64300/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [64400/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [64500/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [64600/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [64700/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [64800/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [64900/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [65000/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [65100/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [65200/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [65300/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [65400/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [65500/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [65600/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [65700/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [65800/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [65900/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [66000/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [66100/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [66200/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [66300/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [66400/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [66500/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [66600/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [66700/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [66800/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [66900/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [67000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [67100/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [67200/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [67300/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [67400/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [67500/100000], Generator Loss: 0.0045, Discriminator Loss: 1.3863\n",
      "Epoch [67600/100000], Generator Loss: 0.0049, Discriminator Loss: 1.3863\n",
      "Epoch [67700/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [67800/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [67900/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [68000/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [68100/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [68200/100000], Generator Loss: 0.0137, Discriminator Loss: 1.3863\n",
      "Epoch [68300/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [68400/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [68500/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [68600/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [68700/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [68800/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [68900/100000], Generator Loss: 0.0056, Discriminator Loss: 1.3863\n",
      "Epoch [69000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [69100/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [69200/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [69300/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [69400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [69500/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [69600/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [69700/100000], Generator Loss: 0.0065, Discriminator Loss: 1.3863\n",
      "Epoch [69800/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [69900/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [70000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [70100/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [70200/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [70300/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [70400/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [70500/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [70600/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [70700/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [70800/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [70900/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [71000/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [71100/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [71200/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [71300/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [71400/100000], Generator Loss: 0.0044, Discriminator Loss: 1.3863\n",
      "Epoch [71500/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [71600/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [71700/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [71800/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [71900/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [72000/100000], Generator Loss: 0.0042, Discriminator Loss: 1.3863\n",
      "Epoch [72100/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [72200/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [72300/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [72400/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [72500/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [72600/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [72700/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [72800/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [72900/100000], Generator Loss: 0.0022, Discriminator Loss: 1.3863\n",
      "Epoch [73000/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [73100/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [73200/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [73300/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [73400/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [73500/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [73600/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [73700/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [73800/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [73900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [74000/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [74100/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [74200/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [74300/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [74400/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [74500/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [74600/100000], Generator Loss: 0.0051, Discriminator Loss: 1.3863\n",
      "Epoch [74700/100000], Generator Loss: 0.0017, Discriminator Loss: 1.3863\n",
      "Epoch [74800/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [74900/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [75000/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [75100/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [75200/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [75300/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [75400/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [75500/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [75600/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [75700/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [75800/100000], Generator Loss: 0.0024, Discriminator Loss: 1.3863\n",
      "Epoch [75900/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [76000/100000], Generator Loss: 0.0046, Discriminator Loss: 1.3863\n",
      "Epoch [76100/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [76200/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [76300/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [76400/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [76500/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [76600/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [76700/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [76800/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [76900/100000], Generator Loss: 0.0072, Discriminator Loss: 1.3863\n",
      "Epoch [77000/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [77100/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [77200/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [77300/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [77400/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [77500/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [77600/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [77700/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [77800/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [77900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [78000/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [78100/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [78200/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [78300/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [78400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [78500/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [78600/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [78700/100000], Generator Loss: 0.0061, Discriminator Loss: 1.3863\n",
      "Epoch [78800/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [78900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [79000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [79100/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [79200/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [79300/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [79400/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [79500/100000], Generator Loss: 0.0027, Discriminator Loss: 1.3863\n",
      "Epoch [79600/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [79700/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [79800/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [79900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [80000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [80100/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [80200/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [80300/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [80400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [80500/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [80600/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [80700/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [80800/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [80900/100000], Generator Loss: 0.0035, Discriminator Loss: 1.3863\n",
      "Epoch [81000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [81100/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [81200/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [81300/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [81400/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [81500/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [81600/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [81700/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [81800/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [81900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [82000/100000], Generator Loss: 0.0020, Discriminator Loss: 1.3863\n",
      "Epoch [82100/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [82200/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [82300/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [82400/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [82500/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [82600/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [82700/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [82800/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [82900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [83000/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [83100/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [83200/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [83300/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [83400/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [83500/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [83600/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [83700/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [83800/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [83900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [84000/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [84100/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [84200/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [84300/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [84400/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [84500/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [84600/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [84700/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [84800/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [84900/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [85000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [85100/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [85200/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [85300/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [85400/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [85500/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [85600/100000], Generator Loss: 0.0039, Discriminator Loss: 1.3863\n",
      "Epoch [85700/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [85800/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [85900/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [86000/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [86100/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [86200/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [86300/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [86400/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [86500/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [86600/100000], Generator Loss: 0.0015, Discriminator Loss: 1.3863\n",
      "Epoch [86700/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [86800/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [86900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [87000/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [87100/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [87200/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [87300/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [87400/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [87500/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [87600/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [87700/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [87800/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [87900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [88000/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [88100/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [88200/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [88300/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [88400/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [88500/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [88600/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [88700/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [88800/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [88900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [89000/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [89100/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [89200/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [89300/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [89400/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [89500/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [89600/100000], Generator Loss: 0.0021, Discriminator Loss: 1.3863\n",
      "Epoch [89700/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [89800/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [89900/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [90000/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [90100/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [90200/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [90300/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [90400/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [90500/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [90600/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [90700/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [90800/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [90900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [91000/100000], Generator Loss: 0.0040, Discriminator Loss: 1.3863\n",
      "Epoch [91100/100000], Generator Loss: 0.0019, Discriminator Loss: 1.3863\n",
      "Epoch [91200/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [91300/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [91400/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [91500/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [91600/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [91700/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [91800/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [91900/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [92000/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [92100/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [92200/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [92300/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [92400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [92500/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [92600/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [92700/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [92800/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [92900/100000], Generator Loss: 0.0016, Discriminator Loss: 1.3863\n",
      "Epoch [93000/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [93100/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [93200/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [93300/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [93400/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [93500/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [93600/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [93700/100000], Generator Loss: 0.0180, Discriminator Loss: 1.3863\n",
      "Epoch [93800/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [93900/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [94000/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [94100/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [94200/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [94300/100000], Generator Loss: 0.0038, Discriminator Loss: 1.3863\n",
      "Epoch [94400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [94500/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [94600/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [94700/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [94800/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [94900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [95000/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [95100/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [95200/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [95300/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [95400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [95500/100000], Generator Loss: 0.0025, Discriminator Loss: 1.3863\n",
      "Epoch [95600/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [95700/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [95800/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [95900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [96000/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [96100/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [96200/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [96300/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [96400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [96500/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [96600/100000], Generator Loss: 0.0018, Discriminator Loss: 1.3863\n",
      "Epoch [96700/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [96800/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [96900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [97000/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [97100/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [97200/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [97300/100000], Generator Loss: 0.0028, Discriminator Loss: 1.3863\n",
      "Epoch [97400/100000], Generator Loss: 0.0009, Discriminator Loss: 1.3863\n",
      "Epoch [97500/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [97600/100000], Generator Loss: 0.0023, Discriminator Loss: 1.3863\n",
      "Epoch [97700/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [97800/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [97900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [98000/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [98100/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [98200/100000], Generator Loss: 0.0014, Discriminator Loss: 1.3863\n",
      "Epoch [98300/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [98400/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [98500/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [98600/100000], Generator Loss: 0.0011, Discriminator Loss: 1.3863\n",
      "Epoch [98700/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [98800/100000], Generator Loss: 0.0032, Discriminator Loss: 1.3863\n",
      "Epoch [98900/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [99000/100000], Generator Loss: 0.0084, Discriminator Loss: 1.3863\n",
      "Epoch [99100/100000], Generator Loss: 0.0005, Discriminator Loss: 1.3863\n",
      "Epoch [99200/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [99300/100000], Generator Loss: 0.0010, Discriminator Loss: 1.3863\n",
      "Epoch [99400/100000], Generator Loss: 0.0013, Discriminator Loss: 1.3863\n",
      "Epoch [99500/100000], Generator Loss: 0.0007, Discriminator Loss: 1.3863\n",
      "Epoch [99600/100000], Generator Loss: 0.0012, Discriminator Loss: 1.3863\n",
      "Epoch [99700/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n",
      "Epoch [99800/100000], Generator Loss: 0.0008, Discriminator Loss: 1.3863\n",
      "Epoch [99900/100000], Generator Loss: 0.0006, Discriminator Loss: 1.3863\n",
      "Epoch [100000/100000], Generator Loss: 0.0004, Discriminator Loss: 1.3863\n"
     ]
    }
   ],
   "source": [
    "# Execution\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    df = pd.read_csv('../data/interpolated_spectra.csv').T\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = range(len(df.columns))\n",
    "    total_rows = len(df)\n",
    "    sequence_length = 17\n",
    "    num_features = 10001\n",
    "\n",
    "    assert df.shape[1] == num_features, f\"Expected {num_features} features, but got {df.shape[1]}\"\n",
    "    if total_rows % sequence_length == 0:\n",
    "        num_samples = total_rows // sequence_length\n",
    "    else:\n",
    "        raise ValueError(\"The total number of rows is not divisible by sequence length. Please adjust the data.\")\n",
    "\n",
    "    data = df.values.astype(np.float64).reshape(num_samples, sequence_length, num_features)\n",
    "    scaler = fit_scaler(data)\n",
    "    train_loader = get_dataloader(data, batch_size=32)\n",
    "\n",
    "    # Train the updated TimeGAN\n",
    "    train_timegan(train_loader, scaler, device, epochs=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\AppData\\Local\\Temp\\ipykernel_9288\\3956648598.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('new_timegan_weights.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8429],\n",
      "         [-0.7056],\n",
      "         [-0.5924],\n",
      "         [-0.5320],\n",
      "         [-0.3808],\n",
      "         [-0.2604],\n",
      "         [-0.1444],\n",
      "         [ 0.0723],\n",
      "         [ 0.1749],\n",
      "         [ 0.2754],\n",
      "         [ 0.4202],\n",
      "         [ 0.9008],\n",
      "         [ 1.3657],\n",
      "         [ 1.4748],\n",
      "         [ 1.7247],\n",
      "         [ 1.9185],\n",
      "         [ 2.0646]]], device='cuda:0')\n",
      "(17,) (17,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Average MSE: 0.0007, Average MAE: 0.0148\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model weights\n",
    "timegan = TimeGAN(1, 64, 32, device)\n",
    "checkpoint = torch.load('new_timegan_weights.pth', map_location=device)\n",
    "timegan.generator_net.load_state_dict(checkpoint['generator_net'])\n",
    "\n",
    "# Get a batch of real data from the data loader\n",
    "real_data_batch = next(iter(train_loader))\n",
    "real_data_batch = real_data_batch[:32].to(device)  # Ensure batch size of 32\n",
    "\n",
    "# Extract the wavelengths (0th column) from the real data batch\n",
    "wavelengths = real_data_batch[0, :, 0:1].unsqueeze(0)  # Take only the first example and add batch dimension\n",
    "print(wavelengths)\n",
    "wavelengths = wavelengths.repeat(32, 1, 1)  # Repeat across batch dimension to match noise\n",
    "\n",
    "# Generate data using both noise and the real wavelengths\n",
    "noise = torch.randn(32, sequence_length, 32).to(device)\n",
    "generated_data_batch = timegan.generator_net(noise, wavelengths)\n",
    "\n",
    "# Visualize Real vs Generated Data\n",
    "plot_real_vs_generated(real_data_batch[:, :, 0:], generated_data_batch, scaler, num_samples=5)\n",
    "# Assuming you already have a DataLoader `real_loader` with your dataset\n",
    "evaluate_model(timegan, train_loader, scaler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Fluxes for Single Galaxy: [[ -4.9972734    5.849107     3.8484647  ... -22.614471     5.303298\n",
      "    0.16740918]\n",
      " [ -4.957427     5.897991     3.9117565  ... -22.7582       5.372359\n",
      "    0.23528847]\n",
      " [ -4.7621403    6.0303125    3.9661243  ... -22.372889     5.5153923\n",
      "    0.27760035]\n",
      " ...\n",
      " [ -1.1618348    7.9681706    5.5287027  ...  13.042184     7.4237795\n",
      "    1.9395081 ]\n",
      " [ -1.1625265    8.047002     5.2150497  ...  10.541235     7.6051207\n",
      "    1.7065983 ]\n",
      " [ -1.3136125    8.035313     4.848559   ...   8.142142     7.673303\n",
      "    1.3823463 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\AppData\\Local\\Temp\\ipykernel_9288\\1172719456.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('new_timegan_weights.pth', map_location=device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x25817e712e0>"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your trained model weights\n",
    "checkpoint = torch.load('new_timegan_weights.pth', map_location=device)\n",
    "timegan = TimeGAN(1, 64, 32, device)\n",
    "timegan.generator_net.load_state_dict(checkpoint['generator_net'])\n",
    "\n",
    "# Example wavelengths to use for prediction (sequence length should match your data)\n",
    "example_wavelengths = torch.tensor([\n",
    "    [0.1], [0.2], [0.3], [0.4], [0.5], [0.6], [0.7], [0.8], [0.9], [1.0],\n",
    "    [1.1], [1.2], [1.3], [1.4], [1.5], [1.6], [1.7]\n",
    "])\n",
    "\n",
    "# Generate flux predictions for a single galaxy\n",
    "generated_fluxes = generate_single_galaxy_flux(timegan, example_wavelengths, scaler, device)\n",
    "print(\"Generated Fluxes for Single Galaxy:\", generated_fluxes)\n",
    "plt.scatter(example_wavelengths,generated_fluxes[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
